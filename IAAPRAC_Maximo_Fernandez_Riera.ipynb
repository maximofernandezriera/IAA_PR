{"cells":[{"cell_type":"markdown","metadata":{"id":"Idffb0thnM1H"},"source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<div style=\"float: right; width: 50%;\">\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M1.304 · Inteligencia Artificial Avanzada / M0.539 · Inteligencia Artificial</p>\n","<p style=\"margin: 0; text-align:right;\">MU Ingeniería Informática / MU Ingeniería Computacional y Matemática</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>"]},{"cell_type":"markdown","metadata":{"id":"1wXip_npz09_"},"source":["## MEJORA DE IMÁGENES UTILIZANDO REDES NEURONALES CONVOLUCIONALES"]},{"cell_type":"markdown","metadata":{"id":"ZfCMCxAZTXtZ"},"source":["### Presentación\n","\n","Una de las ventajas más destacables de las redes neuronales o *Neural Networks* (NN) es el amplio abanico de campos en los que se pueden aplicar. Aunque se pueden utilizar, con resultados destacados, en ámbitos como la clasificación o la recomendación, también se pueden aplicar a campos que van más allá de los problemas clásicos. Uno de estos campos es el de inferir la información que falta en los datos de entrada.\n","\n","En muy poco tiempo algunas NN que logran este objetivo se han hecho notorias: GPT, Gemini o Bard infieren texto, Dall-E, Midjourney o Stable Diffusion infieren imágenes,... El avance en este campo es tan rápido que muy posiblemente cuando leáis este enunciado algunos de los ejemplos anteriores hayan quedado obsoletos y hayan aparecido otros más nuevos y sofisticados. Estas NN se basan en arquitecturas como las *Generative Adversarial Networks* (GAN) o los *Transformers*, que están fuera del alcance de esta actividad. Utilizaremos una arquitectura mucho más sencilla que, a pesar de su simplicidad, produce resultados razonablemente buenos en algunas tareas de inferencia de datos.\n","\n","Más concretamente, aprovecharemos las redes neuronales convolucionales o *Convolutional Neural Networks* (CNN), que son muy adecuadas para trabajar con imágenes, y construiremos un modelo secuencial sencillo. Este modelo se encargará de aumentar la resolución de las imágenes de entrada. Este proceso, conocido como *Super Resolution* implica que la red neuronal debe inferir la información que falta en la imagen de baja resolución."]},{"cell_type":"markdown","metadata":{"id":"u6UhmuehTb8G"},"source":["### Competencias\n","\n","En este enunciado se trabajan las siguientes competencias generales de máster:\n","- Capacidad para proyectar, calcular y diseñar productos, procesos e instalaciones en todos los ámbitos de la ingeniería informática.\n","- Capacidad para el modelado matemático, cálculo y simulación en centros tecnológicos y de ingeniería de empresa, particularmente en tareas de investigación, desarrollo e innovación en todos los ámbitos relacionados con la ingeniería informática.\n","- Capacidad para aplicar los conocimientos adquiridos y solucionar problemas en entornos nuevos o poco conocidos dentro de contextos más amplios y multidisciplinares, siendo capaces de integrar estos conocimientos.\n","- Poseer habilidades para el aprendizaje continuo, autodirigido y autónomo.\n","- Capacidad para modelar, diseñar, definir la arquitectura, implantar, gestionar, operar, administrar y mantener aplicaciones, redes, sistemas, servicios y contenidos informáticos.\n","\n","Las competencias específicas de esta asignatura que se trabajan en esta prueba son:\n","- Entender qué es el aprendizaje automático en el contexto de la inteligencia artificial.\n","- Distinguir entre los diferentes tipos y métodos de aprendizaje.\n","- Aplicar las técnicas estudiadas a un caso real."]},{"cell_type":"markdown","metadata":{"id":"ZuNVYxUITovf"},"source":["### Objetivos\n","\n","En esta práctica aprenderéis a:\n","\n","* Utilizar bibliotecas como Keras, NumPy, skimage o matplotlib.\n","* Crear un generador de datos con capacidades básicas de aumento de datos.\n","* Construir, entrenar y evaluar una CNN enfocada en la mejora de imágenes.\n","* Interpretar la salida de una red neuronal.\n","* Tratar diferentes formatos de imagen."]},{"cell_type":"markdown","metadata":{"id":"6MVVhDnHT_-Q"},"source":["### Recursos\n","\n","Esta práctica requiere los siguientes recursos.\n","\n","Archivos proporcionados:\n","\n","  * Este archivo .ipynb que estás leyendo en este momento.\n","  * La carpeta FACES, que contiene las imágenes que usarás en esta práctica.\n","  * La carpeta MEBeauty, la cual contiene las imágenes que usarás **solo en el ejercicio 5**\n","  * La carpeta IMG, que contiene las imágenes que se visualizan como parte del enunciado. No debes usarlas ni modificarlas para nada.\n","\n","Complementario:\n","\n","  * Materiales de la asignatura.\n","  * Documentación de las bibliotecas. En esta práctica trabajarás principalmente con Keras y Tensorflow, aunque otras bibliotecas como NumPy, skimage o matplotlib te serán útiles. Es recomendable que leas la documentación siempre que lo necesites."]},{"cell_type":"markdown","metadata":{"id":"Pm4AkmzXUWPx"},"source":["### Entrega y criterios de evaluación\n","\n","La práctica debe entregarse dentro del plazo indicado en el Canvas de la asignatura.\n","\n","La entrega debe incluir una versión editada de este cuaderno (.ipynb). Se recomienda el uso de Google Colab (https://colab.research.google.com/). El código de las soluciones a los ejercicios debe implementarse y ejecutarse en las celdas de código proporcionadas y las respuestas justificadas deben agregarse a las celdas de texto correspondientes.\n","\n","Todas las respuestas deben estar correctamente razonadas y justificadas. **Las soluciones que no vayan acompañadas de la correspondiente respuesta razonada no serán evaluadas**.\n","\n","Los ejercicios se valorarán de la siguiente manera:\n","* Ejercicio 1:  2 puntos\n","* Ejercicio 2:  1 punto\n","* Ejercicio 3:  3 puntos\n","* Ejercicio 4:  1.5 puntos\n","* Ejercicio 5:  1.5 puntos\n","* Ejercicio 6:  1 punto\n","\n","Cada ejercicio será evaluado teniendo en cuenta tanto la corrección técnica de la solución como la justificación y argumentación del procedimiento y los resultados.\n","\n","Cada ejercicio está dividido en distintos apartados (a, b, c, ...). Esta división tiene el objetivo de guiarlos en la resolución de la práctica y deben resolver cada apartado, proporcionando el código y las explicaciones requeridas. Sin embargo, estos apartados *no* se evaluarán individualmente. Cada ejercicio será evaluado de manera global, por lo que no hay una puntuación específica para cada apartado."]},{"cell_type":"markdown","metadata":{"id":"lxm-7MuJU6bK"},"source":["### Descripción de la práctica"]},{"cell_type":"markdown","metadata":{"id":"IKs2S1W9BbHe"},"source":["#### Visión general\n","\n","En esta práctica desarrollaréis, entrenaréis y probaréis una red neuronal capaz de aumentar la resolución de una imagen. A continuación podéis ver unos cuantos ejemplos de los resultados que podréis alcanzar con la red neuronal que desarrollaréis en esta práctica.\n","\n","![EJEMPLOS INICIALES](IMG/EXAMPLE.png)"]},{"cell_type":"markdown","metadata":{"id":"XJxgtyEtBbHf"},"source":["#### Los datos\n","\n","En general, una red neuronal es una función con un gran conjunto de parámetros. El entrenamiento de una red neuronal es necesario para encontrar los valores adecuados para todos estos parámetros. En muchos casos, esto se logra proporcionando a la red los datos que debe procesar junto con el resultado esperado del procesamiento o *ground truth*. Una vez entrenada, la red es capaz de procesar datos que no había visto durante el entrenamiento.\n","\n","Nuestra red neuronal se alimentará de imágenes de baja resolución (32x32 píxeles) y proporcionará imágenes de alta resolución (128x128 píxeles). Esto significa que durante el entrenamiento necesitaremos pares de imágenes de 32x32 píxeles y su correspondiente versión de 128x128. Para hacer frente a la necesidad de pares de imágenes de baja y alta resolución utilizaremos un conjunto de imágenes de 128x128 píxeles y generaremos su versión de 32x32. Esto es factible ya que el proceso de reducir la resolución es fácil y rápido.\n","\n","En general, las redes neuronales necesitan *muchos* datos para entrenar. Por esta razón, limitaremos el alcance de nuestra red neuronal de aumento de resolución a rostros humanos, lo que nos permitirá trabajar con un conjunto razonablemente pequeño de datos. En concreto, trabajaremos con el conjunto de datos IMDB-WIKI, que contiene más de 500.000 imágenes de rostros humanos, en su mayoría. El conjunto de datos original se puede encontrar aquí:\n","\n","https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/\n","\n","Este conjunto de datos es demasiado grande si queremos tener tiempos de entrenamiento razonables para esta actividad y, desafortunadamente, tiene algunos errores. Por esta razón, hemos seleccionado aleatoriamente un subconjunto de 5000 imágenes entre aquellas que no tenían imperfecciones o errores. Esto significa que no debéis trabajar con el conjunto de datos original sino con el que se proporciona en la carpeta FACES, que es el que os hemos preparado."]},{"cell_type":"markdown","metadata":{"id":"wUBN2ujsBbHg"},"source":["#### El entorno de trabajo\n","\n","Como se indicó anteriormente, esta práctica se entregará como un archivo ipynb. Podéis trabajar con este archivo localmente a través de la plataforma Jupyter (Jupyter Lab o Jupyter Notebook). En este caso, deberéis instalar Keras y Tensorflow además de bibliotecas como scikit-image, entre otras, en caso de que no las tengan ya instaladas.\n","\n","Para evitar problemas de dependencias entre bibliotecas, de versiones o de conflictos entre diferentes instalaciones en su ordenador, se recomienda trabajar con la plataforma Google Colab:\n","\n","https://colab.research.google.com/notebooks/welcome.ipynb\n","\n","Una vez accedáis al enlace anterior, deberéis identificaros con las credenciales de Google. Podéis usar (y de hecho, recomendamos que lo hagáis) vuestras credenciales de Google de la UOC.\n","\n","En Colab podréis trabajar con el ipynb proporcionado y ya encotraréis Keras, Tensorflow, sklearn y otras bibliotecas instaladas. En caso de necesitar alguna biblioteca que no esté instalada, podéis instalarla desde el propio Colab con el comando:\n","\n","```\n","!pip install biblioteca-a-instalar\n","```\n","\n","Antes de comenzar a trabajar en esta práctica, es muy recomendable que os familiaricéis con Google Colab. En particular, es extremadamente importante que tengáis en cuenta que *todos* los archivos que se generen o almacenen dentro del espacio de Colab se perderán al finalizar o abandonar la sesión. Por lo tanto, deberéis decidir cómo hacer que estos datos sean persistentes si así lo requieren: podéis descargarlos en vuestro disco duro local antes de abandonar la sesión y volver a cargarlos al comenzar de nuevo, o también podéis vincular vuestro Google Drive a Colab para acceder directamente.\n","\n","Tened en cuenta que los accesos entre Colab y Google Drive son lentos. Por lo tanto, no es recomendable que vuestros programas accedan directamente a Google Drive. Es mejor copiar los datos del Drive a Colab al principio de una sesión y desde Colab al Drive al final.\n","\n","Encontraréis información detallada relacionada con estos aspectos en el siguiente enlace:\n","\n","https://colab.research.google.com/notebooks/io.ipynb\n","\n","En cualquier caso, la forma en que gestionéis los archivos es decisión vuestra y forma parte de la práctica. Los profesores no proporcionarán código para gestionar estos aspectos. También cabe destacar que los profesores darán soporte en el aula para Google Colab pero no para otros entornos.\n","\n","Una herramienta útil para desarrollar, entrenar y evaluar redes neuronales es TensorBoard. No hay ninguna pregunta o ejercicio relacionado con TensorBoard en la práctica, aunque podéis usarlo si lo consideráis útil. Sin embargo, independientemente de si usáis TensorBoard o no, deberéis resolver y responder a los ejercicios de la forma exacta en que se solicite. Podéis encontrar información detallada sobre TensorBoard aquí:\n","\n","https://www.tensorflow.org/tensorboard"]},{"cell_type":"markdown","metadata":{"id":"G4j1kdGhBbHg"},"source":["#### Tiempo de entrenamiento\n","\n","Tened en cuenta que entrenar una red neuronal consume mucho tiempo. Si trabajáis localmente, podéis reducir el tiempo de entrenamiento utilizando una GPU. Desafortunadamente, no todas las GPU se pueden utilizar para este propósito ni la instalación de sus controladores es fácil. Los profesores *no* proporcionarán soporte para la instalación de controladores de una GPU, de CUDA, etcétera. Es por este motivo que *no* recomendamos esta opción a menos que ya tengáis una GPU adecuadamente instalada y funcionando con Keras y TensorFlow en vuestro ordenador.\n","\n","Si usáis Colab podréis utilizar las GPU de Google, lo que reducirá significativamente el tiempo de entrenamiento en comparación con un entrenamiento en CPU. Sin embargo, tened en cuenta que los recursos de GPU que proporciona Google en Colab son limitados, tanto en potencia de cálculo como en tiempo de uso, por lo que es posible que no siempre podáis conectaros o que en algún momento se corte la conexión con la GPU. Por lo tanto, se recomienda que utilicéis las GPU de Google solo cuando sea necesario y que trabajéis en modo CPU la mayor parte del tiempo.\n","\n","Los ejercicios propuestos se centran en redes neuronales simples y en entrenamientos pequeños (solo unas pocas épocas o *epochs*). Estos entrenamientos pequeños sobre redes neuronales simples no os permitirán ver todo el potencial del *deep learning*, pero sí os darán algunas pistas sobre cómo funcionan las redes neuronales, así como sus ventajas e inconvenientes. Si, una vez terminada la práctica, queréis repetir todos los entrenamientos durante más épocas o queréis intentar ampliar los modelos simples con los que habéis trabajado, podéis hacerlo (sin que forme parte de la práctica a entregar).\n","\n","Algunos ejercicios requieren funciones, variables o modelos creados y entrenados en ejercicios anteriores. Si abandonáis la sesión (tanto local como remotamente con Colab) para continuar más adelante, deberéis volver a ejecutar todas las celdas. Esto no es problemático excepto para los entrenamientos (ya que tardan tiempo en completarse). Dado que se os pedirá que guardéis modelos entrenados en disco, nuestro consejo es que verifiquéis (en vuestro código) si el modelo a entrenar ya existe en el disco y, si es así, lo carguéis en lugar de volver a entrenarlo. Podéis hacerlo incluso si el ejercicio no lo pide expresamente.\n","\n","En relación con lo anterior, recordad que los archivos del espacio Colab se pierden al cerrar la sesión. Esto incluye los modelos entrenados que hayáis guardado. Para preservarlos, tendréis que utilizar las técnicas mencionadas anteriormente.\n","\n","Como referencia, si trabajáis con una CPU *normal*, podéis esperar tiempos de entrenamiento en los modelos con los que trabajaréis de entre 2 y 3 minutos por epoch. Si disponéis de una GPU *normal* y podéis utilizarla para el entrenamiento, estos tiempos se reducirán a entre 10 y 15 segundos por epoch. En Google Colab con GPU, los tiempos de entrenamiento dependerán mucho de la disponibilidad y la carga de trabajo de los servidores de Google, pero pueden ser similares a los indicados para el caso local."]},{"cell_type":"markdown","metadata":{"id":"UzGMpO4GBbHh"},"source":["#### Sobre Keras\n","\n","Keras ha experimentado algunos cambios recientemente que afectan la forma de importar sus componentes. Si en algún momento intentáis importar un componente de Keras con **import keras.xxx** y aparece un mensaje de error, deberéis cambiar la importación a **import tensorflow.keras.xxx** o viceversa."]},{"cell_type":"markdown","metadata":{"id":"QGkmLXEYU-bp"},"source":["### Ejercicios"]},{"cell_type":"markdown","metadata":{"id":"XB1MHlywBbHh"},"source":["#### **Ejercicio 1: Comprendiendo el conjunto de datos**"]},{"cell_type":"markdown","metadata":{"id":"Z8d8JaswxuL3"},"source":["Antes de comenzar, es conveniente entender qué es una imagen digital y cómo se almacena. Las bibliotecas que usaremos para trabajar con imágenes serán skimage y numpy. Recordad que, a menos que se indique lo contrario, trabajaremos con las imágenes de FACES."]},{"cell_type":"markdown","metadata":{"id":"87SgTGBIBbHh"},"source":["**1-a) Seleccionad una imagen del conjunto de datos y abridla con la función imread de skimage.io. Esta función proporciona una matriz numpy. Imprimid por pantalla su *shape*, su tipo (entero, punto flotante, ...) y sus valores mínimo y máximo. Mostrad la imagen con la función imshow de matplotlib.pyplot.**\n","\n","**Finalmente, responded a las siguientes preguntas: ¿cuántas dimensiones tiene el shape? ¿qué representa cada una de ellas?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClXwWCz4BbHi"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"hH2vOwbRBbHj"},"source":["**Escribid aquí vuestra respuesta**"]},{"cell_type":"markdown","metadata":{"id":"KLydwmJxxuL4"},"source":["**1-b) La red neuronal que crearemos trabajará con imágenes que deben cumplir los siguientes requisitos:**\n","\n","* **Deben ser imágenes en escala de grises.**\n","* **Cada píxel debe estar representado por un *float* que puede tomar valores entre 0 y 1.**\n","* **La imagen debe ser una matriz NumPy con *shape* (altura, anchura, 1).**\n","\n","**Además, para poder entrenar y evaluar la red necesitaremos datos de ejemplo y el resultado esperado o *ground truth*. Los datos de ejemplo serán imágenes de baja resolución y los resultados esperados serán las imágenes correspondientes de alta resolución. En el contexto de esta práctica, las imágenes de baja resolución serán de 32x32 píxeles y las de alta resolución de 128x128 píxeles, ambas cumpliendo con los requisitos enumerados.**\n","\n","**Crea la función *load_image(fileName)* que cargue del disco la imagen especificada por fileName y devuelva dos imágenes. La primera imagen es la imagen en escala de grises resultante de la imagen cargada del disco. Por lo tanto, será una imagen de 128x128 píxeles. La segunda imagen es la misma imagen anterior, pero con una resolución de 32x32 píxeles. Es crítico que ambas imágenes cumplan *todos* los requisitos enumerados anteriormente.**\n","\n","**Una vez creada la función, úsala con una imagen del dataset. Luego, muestra una fila con 3 imágenes. La primera debe ser la imagen en color. La segunda debe ser la imagen en escala de grises de 128x128 y la tercera, la imagen en escala de grises de 32x32. Asegúrate de mostrarlas correctamente (las imágenes en escala de grises deben visualizarse en escala de grises).**\n","\n","**Nota: Como resultado de esta práctica, acabaremos aumentando la resolución de imágenes en color, aunque ahora nos centramos en escala de grises.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYqzhLa8xuL4"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"5P2VZ_GXBbHl"},"source":["**1-c) Durante el entrenamiento de una red neuronal puede ocurrir el sobreajuste o *overfitting*. Cuando esto sucede, la red neuronal comienza a *memorizar* los datos de entrenamiento en lugar de aprender de ellos. Enfrentar el sobreajuste es complicado, pero un buen punto de partida es tener más datos de entrenamiento. Como esto no siempre es posible, un enfoque común es realizar lo que se llama aumento de datos o *data augmentation*.**\n","\n","**El aumento de datos consiste en crear variaciones de los datos existentes de manera realista. Por ejemplo, cada una de las imágenes del conjunto de datos podría ser girada horizontalmente y el resultado sería realista. Por el contrario, girarlas verticalmente no es un buen enfoque, ya que las imágenes del conjunto de datos representan personas y las personas no suelen estar boca abajo. En este ejercicio, modificaréis la función load\\_image para que gire horizontalmente una imagen con cierta probabilidad si así se desea.**\n","\n","**Copiad la función load\\_image del apartado anterior a la siguiente celda y modificadla para que reciba dos parámetros de entrada y tenga el formato load\\_image(fileName,doRandomize). El comportamiento de esta nueva función debe ser el mismo que ya tenía si doRandomize==False. Sin embargo, si doRandomize==True, hay un 50% de probabilidad de que la imagen se voltee horizontalmente.**\n","\n","**Después, llamad a esta función cinco veces con doRandomize=True para una imagen de vuestra elección del conjunto de datos y dibujad las imágenes resultantes en dos filas de 5 imágenes cada una. La primera fila debe contener las imágenes en escala de grises de alta resolución (128x128) y la segunda, las de baja resolución (32x32). Dado que hay un 50% de probabilidades de que, cada vez que se llama a load\\_image, la imagen se gire horizontalmente, es probable que veáis algunas imágenes giradas y otras no. Haced algunas pruebas para aseguraros de que giráis la imagen, aproximadamente, la mitad de las veces.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7uAOTyRvBbHl"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"phz-vSTnBbHm"},"source":["**1-d) Para entrenar una red neuronal, es deseable tener tres conjuntos de datos: el conjunto de entrenamiento o *training set* (utilizado para entrenar la red neuronal), el conjunto de validación o *validation set* (utilizado para evaluar el entrenamiento y ajustar la red neuronal si es necesario) y el conjunto de pruebas o *test set* (utilizado para realizar una evaluación final de la red neuronal). Estos conjuntos deben ser disjuntos.**\n","\n","**En este ejercicio, se les pide que creen la función get\\_data\\_file\\_names() encargada de generar estos tres conjuntos. Más concretamente, esta función debe devolver las tres listas siguientes, donde cada lista contiene rutas completas (absolutas o relativas) a archivos de imagen del conjunto de datos:**\n","\n","* **trainSet: debe contener el 70% de los nombres de archivo existentes en el conjunto de datos.**\n","* **testSet: debe contener el 20% de los nombres de archivo existentes en el conjunto de datos.**\n","* **valSet: debe contener el 10% de los nombres de archivo existentes en el conjunto de datos.**\n","\n","**Estas tres listas deben ser disjuntas (no debe haber nombres de archivo repetidos ni dentro de una lista ni entre ellas), deben tener un orden aleatorio (respecto a todo el conjunto y dentro de ellas) y las tres listas combinadas deben contener todos los nombres de archivos del conjunto de datos.**\n","\n","**Después de programar la función, ejecútenla para crear los tres conjuntos mencionados. Almacenen la salida de la función en tres variables: trainSet, testSet y valSet. Tengan en cuenta que estas variables son listas que contienen nombres de archivo. No deben contener las imágenes, solo los nombres de los archivos (las rutas completas de los archivos, incluida la ruta para acceder a ellos -absoluta o relativa- desde este cuaderno).**\n","\n","**Finalmente, impriman el número de nombres de archivo en cada una de las tres listas y comprueben si corresponden al 70%, 20% y 10% del número total de archivos del conjunto de datos.**\n","\n","**Nota: para obtener el contenido de una carpeta de su ordenador, utilicen la función \"listdir\" de la biblioteca \"os\". Para unir un nombre de archivo y una ruta de una manera independiente al sistema operativo, utilicen la función \"join\" de la biblioteca \"os.path\". Para aleatorizar los datos, utilicen una de las funciones de mezcla existentes (por ejemplo, random.shuffle de NumPy, aunque hay otras).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTQU_DExBbHm"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"sf_jr2hEBbHm"},"source":["#### **Ejercicio 2: Generación de datos**"]},{"cell_type":"markdown","metadata":{"id":"4eQSY8SnBbHm"},"source":["Keras proporciona varios métodos para suministrar datos a una red neuronal durante el entrenamiento, las pruebas y la validación. Estos métodos se llaman generadores de datos o *data generators* y hay varios tipos distintos. Por ejemplo, Keras ya ofrece generadores de datos completamente funcionales que trabajan con imágenes. En este ejercicio, sin embargo, programaremos un generador de datos simple en lugar de utilizar los que ya proporciona Keras, para que puedan aprender cómo funcionan, qué es la generación de datos, cómo realizar el aumento de datos dentro del generador de datos y, por supuesto, saber cómo proceder si en algún momento necesitan un tipo de generación de datos más complejo.\n","\n","En este contexto, definiremos la clase DataGenerator. Esta clase hereda de la clase Sequence y, para poder funcionar como generador de datos, debe definir, al menos, dos métodos:\n","\n","* \\_\\_len\\_\\_(): este método devuelve el número de lotes o *batches* diferentes que el generador de datos es capaz de proporcionar.\n","* \\_\\_getitem\\_\\_(i): este método proporciona un lote o *batch* de datos. Cada elemento del lote contiene una imagen de baja resolución en escala de grises, que será la entrada de la red neuronal, y la correspondiente imagen de alta resolución, que será la salida esperada de la red neuronal o *ground truth*.\n","\n","En el contexto de esta práctica, consideraremos que *alta resolución* son 128x128 píxeles y *baja resolución* son 32x32 píxeles.\n","\n","Aunque no es obligatorio, es recomendable que un generador de datos también proporcione el método on\\_epoch\\_end, que se ejecuta después de cada época o *epoch* durante el entrenamiento."]},{"cell_type":"markdown","metadata":{"id":"Qo17gMs_BbHn"},"source":["**2-a) En este ejercicio debes completar la clase DataGenerator que está parcialmente programada en la siguiente celda. Como puedes ver, solo tienes que escribir unas pocas líneas de \\_\\_getitem\\_\\_ y de plot\\_batch. Debes decidir qué hacen \\_\\_getitem\\_\\_ y plot\\_batch a partir de los comentarios del código y de los métodos que ya están programados.**\n","\n","**Una vez completada la clase, debes ejecutar las cuatro líneas siguientes. Estas líneas crean tres objetos DataGenerator: un DataGenerator de entrenamiento, un DataGenerator de prueba y un DataGenerator de validación. La primera entrada (trainSet, testSet y valSet respectivamente) es la salida de la función get\\_data\\_file\\_names que has programado y ejecutado anteriormente. La última línea simplemente muestra por pantalla el primer lote de trainGenerator. Dado que este generador tiene doRandomize==True, cada vez que lo ejecutes deberías ver resultados diferentes (tanto un orden diferente como una aumentación de datos distinta).**\n","\n","```\n","trainGenerator=DataGenerator(trainSet,doRandomize=True)\n","testGenerator=DataGenerator(testSet,doRandomize=False)\n","valGenerator=DataGenerator(valSet,doRandomize=False)\n","trainGenerator.plot_batch(0)\n","```\n","\n","**Ten en cuenta lo siguiente:**\n","* **Puedes llamar a la función load\\_image creada anteriormente.**\n","* **Ejecutar las cuatro líneas mencionadas puede ayudarte a detectar errores en el código de DataGenerator.**\n","* **Las cuatro líneas mencionadas ya se incluyen al final de la siguiente celda del cuaderno para tu comodidad. No es necesario que las vuelvas a escribir.**\n","* **El generador de datos de entrenamiento tiene la aumentación de datos habilitada (doRandomize=True) mientras que los otros dos no. Esta es una forma habitual de trabajar: queremos entrenar la red con datos tan variados como sea posible (aumentación de datos) pero queremos evaluarla siempre con los mismos datos (por lo tanto, los generadores de validación y prueba no usan aumentación de datos).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4Bes_doBbHn"},"outputs":[],"source":["################################################################################\n","# COMPLETAD EL CÓDIGO SOLICITADO Y EJECUTAD LA CELDA\n","################################################################################\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.utils import Sequence\n","from skimage.io import imread\n","\n","class DataGenerator(Sequence):\n","###############################################################################\n","# CONSTRUCTOR\n","# Input: fileNames   - List of the names of the image files in the dataset.\n","#        batchSize   - Neural Networks are usually not trained using one single\n","#                      data item each time, but using batches of data items.\n","#                      This parameters states how many data items are in a\n","#                      batch.\n","#        doRandomize - Randomize file order at the end of each epoch and\n","#                      apply some basic data augmentation (True/False)\n","###############################################################################\n","    def __init__(self,fileNames,batchSize=10,doRandomize=True):\n","        # Store input parameters\n","        self.fileNames=fileNames.copy()\n","        self.batchSize=batchSize\n","        self.doRandomize=doRandomize\n","\n","        # Get the number of images\n","        self.numImages=len(fileNames)\n","\n","        # Additional initializations\n","        self.on_epoch_end()\n","###############################################################################\n","# CALLED AFTER EACH TRAINING EPOCH\n","###############################################################################\n","    def on_epoch_end(self):\n","        if self.doRandomize:\n","            np.random.shuffle(self.fileNames)\n","\n","###############################################################################\n","# GET THE NUMBER OF BATCHES\n","###############################################################################\n","    def __len__(self):\n","        return int(np.ceil(self.numImages/float(self.batchSize)))\n","\n","###############################################################################\n","# OUTPUT A BATCH OF DATA-LABEL PAIRS. LABEL IS THE HIGH-RES GRAYSCALE IMAGE\n","# WHILST DATA IS THE LOW-RES IMAGE.\n","###############################################################################\n","    def __getitem__(self,theIndex):\n","        X=[]\n","        y=[]\n","\n","        # Compute start and end image indexes in the requested batch\n","        bStart=max(theIndex*self.batchSize,0)\n","        bEnd=min((theIndex+1)*self.batchSize,self.numImages)\n","\n","        # For each image in the batch\n","        for i in range(bStart,bEnd):\n","\n","            # Load the image whose file name in self.fileNames[i] and\n","            # convert it to grayscale with float data type and shape (128,128,1).\n","            # Then, if self.doRandomize is True, randomly flip it horizontally\n","            # with 50% probability. If self.doRandomize==False, keep it without\n","            # changes. Store this image into hiresImage. Then, create a second\n","            # version of this image with a resolution 32x32 and shape (32,32,1).\n","            # Store this image into lowresImage.\n","\n","################# PLACE YOUR CODE BETWEEN THIS LINE ... #######################\n","\n","\n","\n","\n","\n","\n","\n","\n","########################### ... AND THIS LINE  ################################\n","\n","            # Append the ab channels as ground truth\n","            y.append(hiresImage)\n","\n","            # Append the grayscale version as data\n","            X.append(lowresImage)\n","\n","        # Return the data (grayscale image) and the ground truth (color image)\n","        return np.array(X),np.array(y)\n","\n","###############################################################################\n","# PLOTS A BATCH. BELOW EACH LOW-RES GRAYSCALE IMAGE, IT SHOWS THE CORRESPONDING\n","# HIGH-RES GRAYSCALE IMAGE.\n","###############################################################################\n","    def plot_batch(self,theIndex,numCols=5):\n","        # Get the batch\n","        X,y=self.__getitem__(theIndex)\n","        # Get the number of images\n","        numImages=X.shape[0]\n","        # Compute the number of rows\n","        numRows=(numImages//numCols)*2\n","        # Create the figure\n","        theFigure,theAxes=plt.subplots(numRows,numCols,figsize=(10,8))\n","        # Loop for all the images\n","        for idxImage in range(numImages):\n","            # Put the low resolution image image into lowresImage and the high\n","            # resolution one into hiresImage.\n","\n","################# PLACE YOUR CODE BETWEEN THIS LINE ... #######################\n","\n","\n","\n","\n","\n","\n","\n","########################### ... AND THIS LINE  ################################\n","\n","            # Compute the row and column to plot\n","            theRow=(idxImage//numCols)*2\n","            theCol=idxImage%numCols\n","            # Plot the low resolution image\n","            theAxes[theRow,theCol].imshow(lowresImage,cmap='gray')\n","            # Plot the high resolution image\n","            theAxes[theRow+1,theCol].imshow(hiresImage,cmap='gray')\n","            # Do not plot the axes (basically for aesthetic purposes)\n","            theAxes[theRow,theCol].axis('off')\n","            theAxes[theRow+1,theCol].axis('off')\n","\n","trainGenerator=DataGenerator(trainSet,doRandomize=True)\n","testGenerator=DataGenerator(testSet,doRandomize=False)\n","valGenerator=DataGenerator(valSet,doRandomize=False)\n","trainGenerator.plot_batch(0)"]},{"cell_type":"markdown","metadata":{"id":"cuXLYYlLBbHo"},"source":["#### **Ejercicio 3: Creación de una red completamente convolucional (FCN)**"]},{"cell_type":"markdown","metadata":{"id":"Ts5W9WuSBbHo"},"source":["Una red neuronal convolucional o *Convolutional Neural Network (CNN)* es una red que realiza operaciones de convolución y que, típicamente, termina con una serie de capas densas encargadas de llevar a cabo tareas de clasificación, recomendación, regresión, etcétera. Cuando estas capas densas no existen y las operaciones llevadas a cabo por la red son solo convoluciones y, posiblemente, *pooling* o *up-sampling*, tenemos una *Fully Convolutional Network (FCN)*, que no debéis confundir con las *Fully Connected Networks*. El caso más habitual de FCN es aquel en el que la entrada y la salida son imágenes. Así pues, una FCN encaja perfectamente en nuestro caso en el que queremos aumentar la resolución de una imagen y en el que, por lo tanto, la entrada y la salida de la red serán imágenes.\n","\n","La idea subyacente es que la imagen de entrada pasa por distintas operaciones de convolución que van generando distintas matrices derivadas de esta imagen. Luego, estas matrices aumentan de tamaño hasta la resolución deseada y se combinan entre sí para proporcionar una imagen de alta resolución. Los parámetros de las convoluciones, llamados *kernels* o *máscaras*, se aprenden durante el entrenamiento.\n","  \n","Buscad información sobre estas arquitecturas. Buscad también información en la documentación de Keras sobre los modelos *Sequential* y las capas *Conv2D* y *UpSampling2D* entre otros.\n","\n","Una vez hayáis entendido estos conceptos podréis hacer este ejercicio."]},{"cell_type":"markdown","metadata":{"id":"Q2071jVkBbHo"},"source":["---\n","\n","**3-a) Completad el código de la siguiente celda para que el modelo resultante cumpla con los siguientes requisitos:**\n","\n","* **El tamaño de la entrada (input_shape) debe ser *inputShape*, que por defecto vale (32,32,1) y que es el tamaño de una imagen de baja resolución del conjunto de datos (ancho=alto=32). La última dimensión, 1, se debe a que la entrada será una imagen en escala de grises, que contiene un solo canal.**\n","* **El tamaño de la salida debe ser (128,128,1), que es el tamaño de la imagen en alta resolución (ancho=alto=128) y 1 canal ya que la imagen estará en escala de grises. Tened en cuenta que la forma de la salida no se especifica directamente en ningún lugar. Lo que sí podéis hacer es ver cuál es empleando, por ejemplo, el método summary() del modelo. Así podréis ajustar el modelo iterativamente hasta que la salida tenga el tamaño indicado.**\n","* **La función de activación de la última capa debe ser \"sigmoid\".**\n","* **El número total de parámetros que se pueden entrenar debe estar entre 100000 y 150000.**\n","* **El modelo resultante debe representar una arquitectura FCN.**\n","* **En algún o algunos puntos dentro del modelo deberán aumentar el tamaño de 32x32 a 128x128. Esto lo podéis hacer con una o varias capas de UpSampling. Es recomendable que lo hagáis de manera que algunas convoluciones operen con datos de 32x32, otras con datos de 64x64 y otras con datos de 128x128.**\n","\n","**Además, vuestro código debe compilar el modelo utilizando el optimizador \"adam\", el \"mse\" (*mean squared error* o error cuadrático medio) como función de pérdida (*loss* function) y, al menos, \"mae\" (*mean absolute error* o error absoluto medio) como métrica.**\n","\n","**Una vez tengáis vuestro modelo definido y compilado, ejecutad la celda. Veréis el resumen del modelo (theModel.summary()). Entre otros, el resumen os indicará el número de parámetros entrenables, y podréis comprobar si están dentro del intervalo especificado anteriormente. Si no lo están, modificad el modelo para que así sea.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJbuacSlBbHp"},"outputs":[],"source":["################################################################################\n","# COMPLETAD EL CÓDIGO SOLICITADO Y EJECUTAD LA CELDA\n","################################################################################\n","\n","from tensorflow.keras import models\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n","\n","def create_and_compile_model(inputShape=(32,32,1)):\n","    theModel=models.Sequential([\n","\n","############## COMPLETAD EL MODELO ENTRE ESTA LÍNEA... ########################\n","\n","\n","\n","\n","############################# ... Y ESTA  #####################################\n","\n","    ])\n","\n","################ COMPILAD EL MODELO ENTRE ESTA LÍNEA ##########################\n","\n","\n","\n","\n","############################# ... Y ESTA  #####################################\n","\n","\n","    return theModel\n","\n","theModel=create_and_compile_model()\n","theModel.summary()"]},{"cell_type":"markdown","metadata":{"id":"6qpETif0BbHp"},"source":["**3-b) Entrenad el modelo anterior durante 30 *epochs* utilizando trainGenerator para proporcionar datos de entrenamiento y valGenerator para proporcionar datos de validación.**\n","\n","**El método de entrenamiento (fit) devuelve, entre otras cosas, el campo *history*. Almacénalo dentro de la variable trainHistory.**\n","\n","**Durante el entrenamiento veréis las métricas de calidad indicadas en el campo \"metrics\" que habéis definido durante la compilación del modelo y también el valor de la función de pérdida (*loss*). Vuestro objetivo es lograr un MSE de entrenamiento y de validación (la función de pérdida o *loss*) de 0.0016 o inferior. Esto significa que la pérdida en la última *epoch* debe ser 0.0016 o inferior tanto para los datos de entrenamiento como de validación.**\n","\n","**Si el MSE (*loss*) no alcanza este valor, entonces debéis modificar el modelo del ejercicio anterior y volverlo a entrenar hasta lograr el MSE solicitado.**\n","\n","**Si, por algún motivo, detenéis el proceso de entrenamiento, deberéis crear y compilar de nuevo el modelo antes de volver a entrenar. De lo contrario, es posible que el entrenamiento no sea correcto.**\n","\n","**En este ejercicio estáis utilizando implícitamente la mayor parte del trabajo realizado en ejercicios anteriores. Por lo tanto, pueden surgir algunos errores que os obliguen a retroceder y corregir los ejercicios anteriores.**\n","\n","**¡ATENCIÓN! El entrenamiento del modelo con una CPU *estándar* puede requerir de 2 a 3 minutos por *epoch*. El entrenamiento con una GPU convencional puede requerir unos 10 segundos por *epoch*. Un entrenamiento con Google Colab con un kernel GPU puede tardar un tiempo similar al mencionado para el caso de usar una GPU convencional en local, pero depende mucho de la carga de trabajo de los servidores de Google. Antes de iniciar el entrenamiento con 30 épocas, haced algunas pruebas con 1 o 2 épocas para asegurarte de que todo funciona bien.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMGSFmmDBbHp"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"-zygSCqwBbHq"},"source":["**3-c) Cread las funciones save_trained_model(fileName,theModel,trainHistory) y [theModel,trainHistory]=load_trained_model(fileName) encargadas de guardar en disco y cargar desde disco, respectivamente, el modelo entrenado y el trainHistory. Para evitar la sobreescritura accidental, implementad la función save_trained_model de manera que si uno de los archivos que se debe guardar ya existe, no se lleve a cabo el guardado.**\n","\n","**En cuanto a guardar y cargar el modelo entrenado, leed la documentación de Keras. En cuanto a guardar y cargar el trainHistory, nuestra recomendación es que utilicéis las funciones de guardado (dump) y carga (load) del módulo \"pickle\", aunque también se aceptan otras opciones.**\n","\n","**Después de crear las funciones, guardad y cargad vuestro modelo (utilizad THEMODEL como fileName) para comprobar que todo funciona como se esperaba.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caSuWSo5BbHq"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"uQnxj2M8BbHq"},"source":["**3-d) Crea la función plot_history(theHistory), donde theHistory es el historial de entrenamiento del modelo (trainHistory en las celdas anteriores). La función debe crear dos figuras. La primera figura debe mostrar la evolución con las épocas de entrenamiento de los MSE de entrenamiento y validación. Recuerda que el MSE es la función de pérdida que hemos utilizado, por lo que los MSE de entrenamiento y validación se llaman loss y val_loss respectivamente. La segunda figura debe mostrar la evolución de los MAE de entrenamiento y validación con las épocas de entrenamiento. Recuerda que MAE es una de las métricas que debías definir. Deberás investigar la estructura de trainHistory para representar los datos.**\n","\n","**Después de eso, llama a plot_history usando el trainHistory obtenido anteriormente.**\n","\n","**Finalmente, comenta los resultados. ¿Puedes notar el sobreajuste?**\n","\n","**Para evitar tener que entrenar cada vez que reinicies el cuaderno, puedes cargar el modelo guardado si es necesario.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uqvsRL7BbHr"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"iRnBSAqcBbHr"},"source":["**Escribid vuestra respuesta aquí**"]},{"cell_type":"markdown","metadata":{"id":"qgcK_dXNBbHr"},"source":["**3-e) Evalúa el modelo (utilizando el método *evaluate* del propio modelo) con los datos de testGenerator. Imprime los MSE y MAE resultantes y comenta los resultados.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4jquECwBbHr"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"sYISdlayBbHr"},"source":["**Escribe aquí tu respuesta**"]},{"cell_type":"markdown","metadata":{"id":"baV0j0XzBbHs"},"source":["---\n","\n","**3-f) Elija un lote (batch) de testGenerator y haga la predicción usando el modelo entrenado (use el método *predict* del propio modelo). Tenga en cuenta que para tomar un lote debe usar el método \\_\\_getitem\\_\\_ del testGenerator. Este método devuelve los datos (X) y el *ground truth* (y). Al método *predict* solo debe pasarle los datos (X).**\n","\n","**Cree la función plot_prediction(inputBatch, thePrediction, numCols=5), donde inputBatch son los mismos datos que ha introducido en el método *predict* y thePrediction es la predicción que ha obtenido (lo que le ha devuelto *predict*). La función debe dibujar cada imagen en baja resolución de inputBatch y, debajo, la imagen de alta resolución predicha. Se deben mostrar un máximo de numCols imágenes por fila. Tenga en cuenta que esta función es casi idéntica al método plot\\_batch de DataGenerator. Por lo tanto, no dude en copiar/pegar y modificarlo aquí.**\n","\n","**Llame a plot_prediction para representar las imágenes en escala de grises y las predicciones correspondientes que ha hecho.**\n","\n","**Finalmente, repita el proceso pero ahora con un lote de trainGenerator. Compare los resultados con los anteriores. ¿Nota diferencias entre la predicción hecha en datos utilizados durante el entrenamiento y la predicción hecha en datos no vistos durante el entrenamiento?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twVerCf5BbHs"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"6LkC1KFExuME"},"source":["**Escriba aquí su respuesta**"]},{"cell_type":"markdown","metadata":{"id":"WNW4DzdgBbHu"},"source":["#### **Ejercicio 4: Utilizando la red neuronal**"]},{"cell_type":"markdown","metadata":{"id":"ABWSxtlkBbHu"},"source":["**4-a) Programad la función *enhance\\_image(theImage,theModel)*. Esta función recibe una imagen en escala de grises con *shape* (32,32,1) dentro de theImage, el modelo previamente entrenado dentro de theModel y devuelve una imagen en escala de grises con *shape* (128,128,1) correspondiente a la versión con resolución mejorada de theImage. Es decir, la función aplica el modelo a una imagen y devuelve el resultado. Notad que las imágenes también deben ser matrices de flotantes con valores entre 0 y 1.**\n","\n","**Utilizad esta función para mejorar una imagen de vuestra elección que forme parte del conjunto de prueba (test). Notad que esta imagen está en 128x128 píxeles en color. Por lo tanto, previamente la tendréis que convertir a 32x32 píxeles en escala de grises. Podéis usar load_image para hacerlo. Después, mostrad una fila con 3 imágenes. La primera es la imagen original pasada a escala de grises, la segunda es la imagen original con resolución 32x32 y la tercera es la imagen mejorada por el modelo.**\n","\n","**Repetid el proceso, pero ahora utilizando una imagen que forme parte del conjunto de entrenamiento. Comparad los resultados.**\n","\n","**Notad que dentro de trainSet y testSet tenéis los nombres de los archivos de los conjuntos de entrenamiento y de prueba respectivamente.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxWMsrPgBbHv"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"tv5jD_1PxuME"},"source":["**4-b) A estas alturas de la práctica debéis pensar que el enunciado empezó con una falsa promesa: ¿dónde está el color? Por ahora solo hemos trabajado en escala de grises.**\n","\n","**Para introducir el color tendremos en cuenta estos dos aspectos:**\n","* **El ojo humano es razonablemente bueno para percibir intensidades lumínicas, pero no tan bueno para percibir colores.**\n","* **Todo lo que se pueda hacer externamente a la red neuronal se debe hacer externamente a la red neuronal para ahorrarle complejidad y tiempo de entrenamiento.**\n","\n","**Para empezar, tomaremos una imagen en color y separaremos la información de intensidad lumínica (luminancia) de la información de color (crominancia). Para ello ya os proporcionamos dos funciones implementadas: encode_image y decode_image, disponibles en la siguiente celda.**\n","\n","**Utilizad estas funciones para obtener la luminancia y los dos canales de crominancia de una imagen de FACES de vuestra elección. Después, mostrad una fila con 5 imágenes. La primera es la imagen original en color. La segunda es el canal de luminancia. La tercera es el primer canal de crominancia, la cuarta es el segundo canal de crominancia y la quinta es la reconstrucción, con decode_image, de la imagen original. Excepto la primera y la quinta, en color, mostrad las demás como imágenes en escala de grises.**\n","\n","**Comentad las imágenes que habéis mostrado. En concreto:**\n","* **¿Qué es el canal de luminancia? ¿Os resulta familiar lo que veis?**\n","* **La calidad de los canales de crominancia es baja. ¿Por qué creéis que es así? ¿A qué se deben los artefactos que se ven?**\n","* **¿Cómo explicáis que los canales de crominancia tengan tantos artefactos y baja calidad mientras que la reconstrucción final es perfecta?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY9PHjmexuMF"},"outputs":[],"source":["import numpy as np\n","from skimage.color import rgb2gray,rgb2lab,lab2rgb\n","\n","def encode_image(rgbImage):\n","    labImage=rgb2lab(rgbImage)\n","    theLuminance=np.expand_dims(labImage[:,:,0]/100,axis=-1)\n","    firstChrominance=(labImage[:,:,1]+127)/255\n","    secondChrominance=(labImage[:,:,2]+128)/255\n","    return theLuminance,firstChrominance,secondChrominance\n","\n","def decode_image(theLuminance,firstChrominance,secondChrominance):\n","    labImage=np.zeros((theLuminance.shape[0],theLuminance.shape[1],3))\n","    labImage[:,:,0]=theLuminance[:,:,0]*100\n","    labImage[:,:,1]=(firstChrominance*255)-127\n","    labImage[:,:,2]=(secondChrominance*255)-128\n","    return lab2rgb(labImage)\n","\n","################################################################################\n","# ESCRIBID VUESTRO CÓDIGO A PARTIR DE AQUÍ Y EJECUTAD LA CELDA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"Qk4aMcSIBbHv"},"source":["**Escribe aquí tu respuesta**"]},{"cell_type":"markdown","metadata":{"id":"6IEwR9tuBbHv"},"source":["**4-c) Con el fin de aumentar la resolución de las imágenes en color, tendremos en cuenta lo comentado anteriormente: dado que el ojo humano no percibe muy bien los colores, utilizaremos la red neuronal para mejorar la resolución de la luminancia. La resolución de los colores la cambiaremos con algún método clásico básico, lo más simple posible. Y luego combinaremos ambas cosas.**\n","\n","**Programa la función enhance_color_image(theImage, theModel). Esta función recibe dentro de theImage una imagen de 32x32 píxeles en color y dentro de theModel el modelo entrenado. La función hace lo siguiente:**\n","\n","* **Descompone theImage en luminancia y crominancias**\n","* **Mejora la resolución de la luminancia con la red neuronal**\n","* **Aumenta la resolución de los dos canales de crominancia a 128x128 sin ningún tipo de interpolación ni anti-aliasing, con alguna funcionalidad simple y rápida de skimage.**\n","* **Combina la luminancia con resolución aumentada y los dos canales de crominancia de 128x128 en una imagen en color y la retorna.**\n","\n","**Una vez programada la función, toma todas las imágenes que has elegido en otros apartados, redimensiona (por programa) a 32x32, manteniéndolas en color, y mejora la resolución con enhance_color_image. Luego, muestra tres filas de imágenes. En la primera fila deben estar las imágenes originales a 128x128 en color. En la segunda, las imágenes escaladas a 32x32 (en color). Y en la tercera, las imágenes con resolución aumentada (128x128 en color).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueaKbhpqBbHv"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"xlz3KqrixuMF"},"source":["---\n","\n","**4-d) En este apartado veremos cómo se comporta el modelo entrenado con datos que no formaban parte de los conjuntos de datos utilizados. Para ello, busca en la red al menos cinco imágenes, de las cuales una de ellas debe ser la cara de una persona real. Intenta que las demás estén lo más alejadas posible de las imágenes de los conjuntos de datos: un personaje de cómic, un edificio, un animal, un vehículo, ... Luego, reduce todas estas imágenes (con tu editor gráfico preferido) a una resolución de 128x128. Finalmente, repite el apartado anterior pero ahora con estas imágenes. Comenta los resultados obtenidos.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1r1fKNexuMF"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"yySv5lQqxuMF"},"source":["**Escribe tu respuesta aquí**"]},{"cell_type":"markdown","metadata":{"id":"5geiHw5qBbHw"},"source":["#### **Ejercicio 5: Cuantificando los sesgos**"]},{"cell_type":"markdown","metadata":{"id":"MI_miEm-BbHw"},"source":["Nosotros, los humanos, tenemos preferencias basadas en nuestras propias experiencias. Por eso tenemos libros, platos o películas preferidas. A veces, estas preferencias conducen a sesgos no deseados. Por ejemplo, los prejuicios de género o étnicos. Las redes neuronales también pueden mostrar sesgos. Es posible que hayas leído sobre algunos *chatbots* basados en IA que se han tenido que cerrar debido a sus declaraciones étnicas y de género o sobre cómo OpenAI tuvo que incluir restricciones éticas en sus modelos de lenguaje GPT-3 y GPT-4 para evitar estos comportamientos indeseables.\n","\n","De la misma manera que los humanos, las *preferencias* de la red neuronal provienen de sus experiencias. En modelos como el que hemos desarrollado, la experiencia proviene solo del proceso de entrenamiento. Por lo tanto, si nuestra red neuronal está sesgada, es porque el conjunto de datos ya lo estaba en primer lugar. Dado que el conjunto de datos proviene de la actividad humana, esto significa que las preferencias y los sesgos humanos pueden *trasladarse* a las redes neuronales.\n","\n","Imaginemos una Red Neuronal destinada a clasificar imágenes de animales. Imaginemos también que los datos de entrenamiento son una muestra aleatoria de imágenes de animales tomadas de lugares de Internet aleatorios. Es fácil ver que la Red Neuronal estará claramente sesgada hacia los gatos, porque como todos sabemos, Internet está hecha básicamente de imágenes de gatos. Esto significa que esta red neuronal funcionaría mejor con gatos y peor con otros animales. Por supuesto, esto es una broma, pero no dista mucho de la realidad.\n","\n","Nuestro conjunto de datos proviene de *Internet Movies Database* (IMDB). Esto significa que la mayoría de las imágenes representan actores, probablemente actores de Hollywood, y esto sugiere posibles sesgos en los datos. Puede haber sesgos de edad (más actores jóvenes que mayores), de apariencia (los estándares de belleza de Hollywood), de género, étnicos, ...\n","\n","En el siguiente ejercicio, evaluarán experimentalmente si los posibles sesgos de género y etnia en el conjunto de datos tienen un impacto visible en el modelo de aumento de resolución que hemos desarrollado. Para ello, trabajarán con el conjunto de datos MEBeauty (https://github.com/fbplab/MEBeauty-database). Hemos preparado una versión de este conjunto de datos que se proporciona en la carpeta MEBeauty. En esta versión hemos eliminado algunas imágenes que originalmente estaban en escala de grises, hemos seleccionado aleatoriamente el mismo número de imágenes por categoría y hemos redimensionado las imágenes a 128x128. Por lo tanto, no deben utilizar el conjunto de datos original sino el que ofrecemos en MEBeauty. Por supuesto, no duden en explorar el enlace anterior para conocer los datos originales.\n","\n","Los datos de MEBeauty se estructuran en carpetas. El nombre de cada carpeta especifica el género y la etnia de las imágenes que contiene. Por ejemplo, la carpeta female/indian contiene retratos de mujeres indias y la carpeta male/asian contiene imágenes de hombres asiáticos. Dediquen unos minutos a explorar las imágenes y familiarizarse con la estructura del conjunto de datos."]},{"cell_type":"markdown","metadata":{"id":"4u0Ud1lVBbHw"},"source":["**5-a) Cree la función build_generators. La función debe explorar la carpeta MEBeauty y generar una lista de listas. Cada lista interna debe tener dos elementos: la ruta a una carpeta terminal (por ejemplo, MEBeauty/female/caucasian) y un objeto DataGenerator instanciado con los archivos de esta carpeta. Cree todos estos generadores de datos con doRandomize=False. Tenga en cuenta que, dado que solo se quieren carpetas terminales (carpetas que no contienen otras carpetas en su interior), la lista externa debe tener una longitud de 12 (6 etnias femeninas y 6 etnias masculinas).**\n","\n","**A continuación, ejecute la función y guarde la lista de salida en una variable llamada allGenerators.**\n","\n","**Nota: la función \"walk\" de la biblioteca \"os\" les ayudará.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxCvHlToBbHx"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"CM9RI3cLBbHx"},"source":["**5-b) Cargad el modelo entrenado y evaluadlo utilizando como datos de prueba cada uno de los generadores de datos de allGenerators. Para cada evaluación, imprimid la ruta correspondiente que habéis almacenado antes (por ejemplo, female/hispanic), el MSE (que es la función de pérdida o loss) y el MAE. Así, por ejemplo, una de las salidas impresas podría ser:**\n","\n","```\n","MEBeauty/male/black, MSE: 0.0018, MAE: 0.0284\n","```\n","\n","**Después, programad lo necesario para detectar categorías atípicas u *outliers*. Es decir, detectad si una categoría concreta (female, mideastern, hispanic, male, ...) tiene un MSE demasiado grande o demasiado pequeño respecto al MSE global. En otras palabras, debéis obtener los datos que os ayudarán a responder el siguiente apartado.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YK7XSc56BbHx"},"outputs":[],"source":["################################################################################\n","# ESCRIBID VUESTRO CÓDIGO EN ESTA CELDA Y EJECUTADLA\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"A1Yh2dn1BbHx"},"source":["**5-c) Analicen los resultados obtenidos. ¿Observan algún sesgo de género o étnico en los resultados? En caso afirmativo, ¿qué harían para prevenirlo? Si no, ¿creen que esto significa que el conjunto de datos no está sesgado en absoluto o simplemente no está sesgado para esta aplicación específica (aumento de resolución de imágenes)?**"]},{"cell_type":"markdown","metadata":{"id":"NykY7O-tBbHy"},"source":["**Escriba su respuesta aquí**"]},{"cell_type":"markdown","metadata":{"id":"JlQzzDLTBbHy"},"source":["#### **Ejercicio 6: Más allá del modelo**\n","\n","En este ejercicio no tienes que programar. Solo debes buscar información y reflexionar sobre los resultados obtenidos y tus hallazgos para responder a las preguntas."]},{"cell_type":"markdown","metadata":{"id":"gFwU3SPExuMG"},"source":["**6-a) Hemos creado, entrenado y evaluado un modelo que mejora razonablemente bien retratos de personas. Ahora bien, el modelo no se comporta tan bien con otros tipos de imágenes. Creéis que podríamos utilizar el mismo modelo y entrenarlo con un dataset más grande que incluyera todo tipo de imágenes (no sólo retratos) con el fin de obtener un sistema capaz de aumentar la resolución de todo tipo de imágenes?**"]},{"cell_type":"markdown","metadata":{"id":"-haald2YBbHy"},"source":["**Escribid aquí vuestra respuesta**"]},{"cell_type":"markdown","metadata":{"id":"ivVdiDWvBbHy"},"source":["**6-b) Nos centramos en los casos en que el modelo funciona bien: retratos de personas. Vamos a suponer que queremos aumentar la resolución de un vídeo en el que se ve a una persona hablando. Para ello, utilizamos el modelo que hemos desarrollado y entrenado para mejorar cada uno de los frames del vídeo individualmente. ¿Creen que el resultado sería bueno? Justifiquen su respuesta.**"]},{"cell_type":"markdown","metadata":{"id":"FEjf8F4pBbHz"},"source":["**Escribid la respuesta aquí**"]},{"cell_type":"markdown","metadata":{"id":"4HjoOX28BbHz"},"source":["**6-c) Buscad información sobre redes neuronales de aumento de resolución. Comentad vuestros hallazgos. ¿Funcionan con cualquier tipo de imagen? ¿Tratan el color de forma separada como lo hemos hecho nosotros?**"]},{"cell_type":"markdown","metadata":{"id":"wZhzOxhiBbHz"},"source":["**Escriba aquí su respuesta**"]},{"cell_type":"markdown","metadata":{"id":"vlnkRb4CiDsR"},"source":["### Nota: Propiedad intelectual\n","\n","A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por tanto comprensible hacerlo en el marco de una práctica de los estudios del Máster en Informática, siempre y cuando esto se documente claramente y no suponga plagio en la práctica.\n","\n","Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se presentará junto con ella un documento en el que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y el su estatus legal: si la obra está protegida por copyright o se acoge a alguna otra licencia de uso (Creative Commons, GNU, GPL ...). El estudiante deberá asegurarse de que la licencia que sea no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente deberá asumir que la obra está protegida por copyright.\n","\n","Deberán, además, adjuntar los archivos originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}